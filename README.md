# Neural Network DSL Compiler (Work-in-Progress)

This project is a toy compiler for a Domain Specific Language (DSL) that describes simple neural network operations (Tensor, Matmul, Relu, Softmax, etc.). The compiler frontend is implemented using Flex (lexer) and Bison (parser) in C++.

---

## üìÇ Project Structure

- **lexer.l**: Lexical analyzer (Flex) - tokenizes input source.
- **parser.y**: Parser (Bison) - defines grammar rules and actions.
- **ast.hpp**: AST node definitions (to be implemented later).
- **README.md**: Project documentation.

---

## üîÑ Compilation Flow

### Lexing (`lexer.l`)
- Scans the input text and produces tokens.
- **Tokens include:**
    - **Keywords:** `MODEL`, `TENSOR`, `MATMUL`, `RELU`, `SOFTMAX`, `PRINT`
    - **Symbols:** `{ } [ ] ( ) , = +`
    - **Identifiers:** `ID`
    - **Numbers:** `NUM`
- Uses the `parser.tab.h` header (generated by Bison) for token definitions.

### Parsing (`parser.y`)
- Consumes the token stream from the lexer.
- Implements the DSL grammar:
    - A program starts with `MODEL { ... }`.
    - Inside the block, you can declare tensors, assign operations, or print results.
- Grammar rules map into semantic actions (currently `cout` debug messages).
- **Future step:** build an Abstract Syntax Tree (AST).
### AST Nodes (`ast.hpp`)
- Placeholder for AST node classes (to be implemented).
- Will represent constructs like `Program`, `TensorDecl`, `Assign`, `Matmul`, `Relu`, `Softmax`, `Print`, etc.

### Intermediate Representation (IR) (`ir_builder.cpp`)
- Currently a stub that prints "=== IR DUMP ===".
- Future step: convert AST to a simple IR for further processing.
  
### Driver (main inside `parser.y`)
- Calls `yyparse()` to start parsing.
- On success, prints `=== AST ===` (once the AST is hooked up).

---

## ‚öôÔ∏è Build Instructions

Make sure you have Flex and Bison installed.

```bash
# Generate lexer
flex lexer.l

# Generate parser
bison -d parser.y

# Compile everything
 g++ parser.tab.cpp lex.yy.cpp ir_builder.cpp -o mycompiler -std=c++17
```

---

## ‚ñ∂Ô∏è Running

Create a sample DSL file, e.g., `model.nn`:

```text
MODEL {
        TENSOR A[2,2]
        TENSOR B[2,2]
        C = MATMUL(A, B)
        D = RELU(C)
        PRINT(D)
}
```

Run:

```bash
./mycompiler < test.nn
```

**Expected output (current stage, after  IR):**
```
=== AST ===
Program
  TensorDecl X [784,1]
  TensorDecl W [128,784]
  Assign Z =
    Matmul(W, X)
  Print Z
=== IR DUMP ===
#1 : TensorDecl name='X' inputs=[] shape=784x1 flops=0
#2 : TensorDecl name='W' inputs=[] shape=128x784 flops=0
#3 : MatMul name='Z' inputs=[W,X] shape=128x1 flops=200704
#4 : Print name='' inputs=[Z] shape=128x1 flops=0
```
g++ parser.tab.cpp lex.yy.cpp ir_builder.cpp -o mycompiler -std=c++17

./mycompiler < model.nn 


## ‚úÖ Current Status

- The lexer and parser are functional.
- The grammar supports:
    - Tensor declarations
    - Assignments with operations
    - Print statements
- Debug outputs show rule recognition.
